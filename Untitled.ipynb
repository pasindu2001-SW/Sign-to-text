{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eaf9e2-7e97-4dd1-ab5e-e08eb4fd6940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.18.0 in c:\\campus\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: opencv-python in c:\\campus\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in c:\\campus\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: scikit-learn in c:\\campus\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\campus\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\campus\\lib\\site-packages (from tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.31.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\campus\\lib\\site-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: jax in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: jaxlib in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\campus\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\campus\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\campus\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\campus\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\campus\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\campus\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\campus\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\campus\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\campus\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\campus\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\campus\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.45.1)\n",
      "Requirement already satisfied: pycparser in c:\\campus\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: rich in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\campus\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\campus\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\campus\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\campus\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.18.0 opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e2d039-334f-4228-8938-06be1609221f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\campus\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\campus\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\campus\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b14b126c-8a75-4818-ab84-d5ca3124ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import seaborn \n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b14ef99-455f-4f14-9120-b40e16e5e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "NUM_CLASSES = 5  # Adjust as per your subset\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "IMAGE_SIZE = (160, 160)\n",
    "N_FRAMES = 20  # Number of frames per video\n",
    "FRAME_STEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cc09930-f0b1-4538-96bd-d1104e92570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Pad and resize an image from a video, and ensure it's in the correct format for grayscale.\n",
    "    \n",
    "    Args:\n",
    "      frame: Grayscale image that needs to be resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted grayscale frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    # Ensure frame is in float32 format\n",
    "    frame = tf.cast(frame, tf.float32)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    frame = frame / 255.0\n",
    "    \n",
    "    # Add channel dimension if it's missing\n",
    "    if len(frame.shape) == 2:\n",
    "        frame = tf.expand_dims(frame, axis=-1)\n",
    "    \n",
    "    # Resize and pad\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (160, 160), frame_step = 15):\n",
    "    \"\"\"\n",
    "    Creates grayscale frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      A NumPy array of grayscale frames in the shape of (n_frames, height, width, 1).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = int(video_length - need_length)\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        result.append(format_frames(gray_frame, output_size))\n",
    "\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame = format_frames(gray_frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    src.release()\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "    \n",
    "class FrameGenerator:\n",
    "    def __init__(self, path, n_frames, training=False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "        Args:\n",
    "          path: Video file paths.\n",
    "          n_frames: Number of frames. \n",
    "          training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.mov')) + list(self.path.glob('*/*.mp4'))\n",
    "        classes = [p.parent.name for p in video_paths] \n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames = frames_from_video_file(path, self.n_frames) \n",
    "            label = self.class_ids_for_name[name] # Encode labels\n",
    "            yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c87af13-9c89-4b1f-9a1b-35879ee1a8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: 40\n",
      "Training CSV samples: 24\n",
      "Validation CSV samples: 8\n",
      "Test CSV samples: 8\n",
      "CSV files have been successfully moved to their respective directories.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to your CSV dataset\n",
    "csv_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\Dataset - MP - CSV')\n",
    "\n",
    "# Find all CSV files recursively\n",
    "csv_files = list(csv_dir.rglob(\"*.csv\"))\n",
    "\n",
    "# Print found CSV files for debugging\n",
    "print(f\"Found CSV files: {len(csv_files)}\")\n",
    "\n",
    "# Check if there are any CSV files\n",
    "if len(csv_files) == 0:\n",
    "    print(\"No CSV files found. Please check the file paths.\")\n",
    "else:\n",
    "    # Split into training, validation, and test sets\n",
    "    train_csv_files, temp_csv_files = train_test_split(csv_files, test_size=0.4, random_state=42)\n",
    "    val_csv_files, test_csv_files = train_test_split(temp_csv_files, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Print out the number of files in each split\n",
    "    print(f\"Training CSV samples: {len(train_csv_files)}\")\n",
    "    print(f\"Validation CSV samples: {len(val_csv_files)}\")\n",
    "    print(f\"Test CSV samples: {len(test_csv_files)}\")\n",
    "\n",
    "    # Define directories for the splits\n",
    "    base_split_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\split')\n",
    "    train_dir = base_split_dir / 'train'\n",
    "    val_dir = base_split_dir / 'val'\n",
    "    test_dir = base_split_dir / 'test'\n",
    "\n",
    "    # Ensure that the directories exist, or create them\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    val_dir.mkdir(parents=True, exist_ok=True)\n",
    "    test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Function to move or copy files to the target directory\n",
    "    def move_files(file_list, target_dir):\n",
    "        for file in file_list:\n",
    "            shutil.copy(file, target_dir)  # Change to shutil.move if you want to move instead of copy\n",
    "\n",
    "    # Move CSV files to corresponding directories\n",
    "    move_files(train_csv_files, train_dir)\n",
    "    move_files(val_csv_files, val_dir)\n",
    "    move_files(test_csv_files, test_dir)\n",
    "\n",
    "    print(\"CSV files have been successfully moved to their respective directories.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7018dba4-7c57-4fb6-b266-75676a6f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained I3D model (or download one)\n",
    "def build_i3d_model(num_classes):\n",
    "    base_model = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "        pooling='avg'  # Global Average Pooling for reducing the dimensions\n",
    "    )\n",
    "    \n",
    "    # Adding temporal dimension using TimeDistributed layers\n",
    "    inputs = tf.keras.layers.Input(shape=(N_FRAMES, IMAGE_SIZE[0], IMAGE_SIZE[1], 1))\n",
    "\n",
    "    # Convert grayscale frames to 3 channels\n",
    "    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(3, (3, 3), padding=\"same\"))(inputs)\n",
    "\n",
    "    # Feed into I3D model (Inflating ConvNet layers)\n",
    "    x = tf.keras.layers.TimeDistributed(base_model)(x)\n",
    "\n",
    "    # Temporal pooling to aggregate frame-level features\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Classification layer\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.85)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae6c5253-94bf-483c-8a49-b16c9269d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(train_path, val_path, batch_size, n_frames):\n",
    "    # Initialize frame generators for training and validation datasets\n",
    "    train_gen = FrameGenerator(train_path, n_frames, training=True)\n",
    "    val_gen = FrameGenerator(val_path, n_frames, training=False)\n",
    "\n",
    "    # Create TensorFlow Dataset from generators\n",
    "    train_dataset = tf.data.Dataset.from_generator(train_gen,\n",
    "                                                   output_signature=(\n",
    "                                                       tf.TensorSpec(shape=(n_frames, 160, 160, 1), dtype=tf.float32),\n",
    "                                                       tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "    val_dataset = tf.data.Dataset.from_generator(val_gen,\n",
    "                                                 output_signature=(\n",
    "                                                     tf.TensorSpec(shape=(n_frames, 160, 160, 1), dtype=tf.float32),\n",
    "                                                     tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "    \n",
    "    # Apply batching and prefetching to optimize performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Define the function to compile the I3D model\n",
    "def compile_i3d_model(model):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "388dfa44-74c9-4c39-9fb9-faa9461efa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping and ModelCheckpoint callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitors validation loss\n",
    "    patience=7,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'fine_tuned_i3d_ucf101_ssl400.weights.h5',  # Change filename extension\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True  # Ensures only weights are saved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f68375b-d416-42f7-aea9-bf90f53d99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "train_dataset, val_dataset = prepare_datasets(train_dir, val_dir, BATCH_SIZE, N_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b588044f-f46b-4b29-abc6-cd5dbd319a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_i3d_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28b8375e-eb4c-4629-8a49-7e1ce6eaab4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,125</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m1\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,   │            \u001b[38;5;34m30\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │    \u001b[38;5;34m21,802,784\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m5,125\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,906,115</span> (91.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,906,115\u001b[0m (91.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,871,683</span> (91.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,871,683\u001b[0m (91.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,432</span> (134.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m34,432\u001b[0m (134.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ad6820b-53b0-4d27-8380-3d2e405838f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy',  # or 'categorical_crossentropy' if one-hot encoded labels\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd802a7e-689a-4335-99c7-f46bc1b14f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 5s/step\n",
      "Epoch 2/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 321ms/step\n",
      "Epoch 3/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 4/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 5/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 6/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 7/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 8/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 9/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 10/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 11/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 12/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 13/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 14/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 15/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 16/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 17/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 18/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 19/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 20/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 21/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 22/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 23/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 24/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 25/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 26/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 27/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 28/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 29/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 30/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 31/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 32/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 33/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 34/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 35/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 36/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 37/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 38/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 39/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 40/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 41/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 42/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 43/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 44/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 45/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 46/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 47/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 48/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 49/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 50/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 51/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 52/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 53/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 54/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 55/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 56/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 57/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 58/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 59/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 60/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 61/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 62/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 63/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 64/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 65/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 66/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 67/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 68/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 69/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 70/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 71/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 72/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 73/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 74/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 75/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 76/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 77/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 78/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 79/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 80/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 81/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 82/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 83/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 84/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 85/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 86/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 87/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 88/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 89/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 90/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 91/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 92/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 93/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 94/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 95/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 96/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 97/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 98/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 99/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 100/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 100  # Set a reasonable fixed number of steps\n",
    "validation_steps = 20   # Define this variable before using it\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,  # Ensure dataset size is known\n",
    "    validation_steps=validation_steps,  # Now it's defined\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "print(history.history.keys())  # Check available training history keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5aaac33e-0f92-4c71-ba7d-178c255eccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([])\n",
      "Best Training Accuracy not available\n",
      "Best Validation Accuracy not available\n",
      "Best Training Loss not available\n",
      "Best Validation Loss not available\n"
     ]
    }
   ],
   "source": [
    "# Check available keys\n",
    "print(history.history.keys())\n",
    "\n",
    "# If 'loss' is available, proceed as normal\n",
    "if 'loss' in history.history:\n",
    "    best_train_loss = min(history.history['loss'])\n",
    "else:\n",
    "    best_train_loss = None  # Handle the case when loss is not available\n",
    "\n",
    "# If 'val_loss' is available, proceed as normal\n",
    "if 'val_loss' in history.history:\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "else:\n",
    "    best_val_loss = None  # Handle the case when val_loss is not available\n",
    "\n",
    "# Similarly for accuracy\n",
    "if 'accuracy' in history.history:\n",
    "    best_train_acc = max(history.history['accuracy'])\n",
    "else:\n",
    "    best_train_acc = None  # Handle the case when accuracy is not available\n",
    "\n",
    "if 'val_accuracy' in history.history:\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "else:\n",
    "    best_val_acc = None  # Handle the case when val_accuracy is not available\n",
    "\n",
    "# Print the results\n",
    "print(f'Best Training Accuracy: {best_train_acc:.4f}' if best_train_acc is not None else 'Best Training Accuracy not available')\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.4f}' if best_val_acc is not None else 'Best Validation Accuracy not available')\n",
    "print(f'Best Training Loss: {best_train_loss:.4f}' if best_train_loss is not None else 'Best Training Loss not available')\n",
    "print(f'Best Validation Loss: {best_val_loss:.4f}' if best_val_loss is not None else 'Best Validation Loss not available')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8421c3f3-22ac-4d2e-9a82-7a4d590160ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m   ax2\u001b[38;5;241m.\u001b[39mlegend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 37\u001b[0m \u001b[43mplot_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 15\u001b[0m, in \u001b[0;36mplot_history\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Plot loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1850x1050 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "    Plotting training and validation learning curves.\n",
    "\n",
    "    Args:\n",
    "      history: model history with all the metric measures\n",
    "  \"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "  fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  \n",
    "  # Determine upper bound of y-axis\n",
    "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
    "\n",
    "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation']) \n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_ylim([0, 1])\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b428cf7-d962-4f4f-8f5e-c9d85aa8c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset): \n",
    "  \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "  \"\"\"\n",
    "  actual = [labels for _, labels in dataset.unbatch()]\n",
    "  predicted = model.predict(dataset)\n",
    "\n",
    "  actual = tf.stack(actual, axis=0)\n",
    "  predicted = tf.concat(predicted, axis=0)\n",
    "  predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "  return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59042df0-0def-4a83-89dc-78aeab31567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(12, 12)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7358fb92-6b1e-419e-aec1-642811c76434",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iterdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fg \u001b[38;5;241m=\u001b[39m \u001b[43mFrameGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(fg\u001b[38;5;241m.\u001b[39mclass_ids_for_name\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[1;32mIn[44], line 86\u001b[0m, in \u001b[0;36mFrameGenerator.__init__\u001b[1;34m(self, path, n_frames, training)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_frames \u001b[38;5;241m=\u001b[39m n_frames\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir()))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_ids_for_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((name, idx) \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'iterdir'"
     ]
    }
   ],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], N_FRAMES, training=True)\n",
    "labels = list(fg.class_ids_for_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e84f77f4-a8e5-40b7-8838-e5de8ec94695",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m actual, predicted \u001b[38;5;241m=\u001b[39m \u001b[43mget_actual_predicted_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plot_confusion_matrix(actual, predicted, labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[75], line 12\u001b[0m, in \u001b[0;36mget_actual_predicted_labels\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m  Create a list of actual ground truth values and the predictions from the model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Ground truth and predicted values for a particular dataset.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m actual \u001b[38;5;241m=\u001b[39m [labels \u001b[38;5;28;01mfor\u001b[39;00m _, labels \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39munbatch()]\n\u001b[1;32m---> 12\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m actual \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack(actual, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m predicted \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(predicted, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\campus\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\campus\\lib\\site-packages\\keras\\src\\utils\\progbar.py:119\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    116\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     numdigits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    120\u001b[0m     bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(numdigits) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (current, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    121\u001b[0m     bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[0m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "actual, predicted = get_actual_predicted_labels(train_dataset)\n",
    "plot_confusion_matrix(actual, predicted, labels, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f19e82-c7f4-417a-96d9-9a3cd2fc7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_gen = FrameGenerator(subset_paths['test'], N_FRAMES, training=False)\n",
    "test_dataset = tf.data.Dataset.from_generator(test_gen,\n",
    "                                              output_signature=(\n",
    "                                                  tf.TensorSpec(shape=(N_FRAMES, 160, 160, 1), dtype=tf.float32),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647e416-2204-45be-860e-640ede5e24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save('fine_tuned_i3d_ucf101_ssl400.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29664b09-4740-4a9f-a9a6-445d5d291f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "# Function to calculate and print precision, recall, F1 score, and accuracy\n",
    "def print_metrics(actual, predicted):\n",
    "    actual_np = actual.numpy()\n",
    "    predicted_np = predicted.numpy()\n",
    "    \n",
    "    precision = precision_score(actual_np, predicted_np, average='weighted')\n",
    "    recall = recall_score(actual_np, predicted_np, average='weighted')\n",
    "    f1 = f1_score(actual_np, predicted_np, average='weighted')\n",
    "    accuracy = accuracy_score(actual_np, predicted_np)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Example usage\n",
    "fg = FrameGenerator(subset_paths['train'], N_FRAMES, training=True)\n",
    "labels = list(fg.class_ids_for_name.keys())\n",
    "\n",
    "# Assuming `train_dataset` is already prepared\n",
    "actual, predicted = get_actual_predicted_labels(train_dataset)\n",
    "print_metrics(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f346e-4109-46b2-82dc-6ea5747b518d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909c64c-4793-4136-8d68-a7db1a932a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cef557-dc08-420c-aa79-fa327bd10b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
