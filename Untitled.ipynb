{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eaf9e2-7e97-4dd1-ab5e-e08eb4fd6940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.18.0 in c:\\campus\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: opencv-python in c:\\campus\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in c:\\campus\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: scikit-learn in c:\\campus\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\campus\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\campus\\lib\\site-packages (from tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\campus\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.31.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\campus\\lib\\site-packages (from mediapipe) (25.1.0)\n",
      "Requirement already satisfied: jax in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: jaxlib in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\campus\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\campus\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\campus\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\campus\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\campus\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\campus\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\campus\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\campus\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\campus\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\campus\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\campus\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\campus\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\campus\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.45.1)\n",
      "Requirement already satisfied: pycparser in c:\\campus\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: rich in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\campus\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\campus\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\campus\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\campus\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\campus\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\campus\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\campus\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.18.0 opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e2d039-334f-4228-8938-06be1609221f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\campus\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\campus\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\campus\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\campus\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b14b126c-8a75-4818-ab84-d5ca3124ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import seaborn \n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b14ef99-455f-4f14-9120-b40e16e5e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "NUM_CLASSES = 5  # Adjust as per your subset\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "IMAGE_SIZE = (160, 160)\n",
    "N_FRAMES = 20  # Number of frames per video\n",
    "FRAME_STEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cc09930-f0b1-4538-96bd-d1104e92570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Pad and resize an image from a video, and ensure it's in the correct format for grayscale.\n",
    "    \n",
    "    Args:\n",
    "      frame: Grayscale image that needs to be resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted grayscale frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    # Ensure frame is in float32 format\n",
    "    frame = tf.cast(frame, tf.float32)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    frame = frame / 255.0\n",
    "    \n",
    "    # Add channel dimension if it's missing\n",
    "    if len(frame.shape) == 2:\n",
    "        frame = tf.expand_dims(frame, axis=-1)\n",
    "    \n",
    "    # Resize and pad\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (160, 160), frame_step = 15):\n",
    "    \"\"\"\n",
    "    Creates grayscale frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      A NumPy array of grayscale frames in the shape of (n_frames, height, width, 1).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = int(video_length - need_length)\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        result.append(format_frames(gray_frame, output_size))\n",
    "\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame = format_frames(gray_frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    src.release()\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "    \n",
    "class FrameGenerator:\n",
    "    def __init__(self, path, n_frames, training=False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "        Args:\n",
    "          path: Video file paths.\n",
    "          n_frames: Number of frames. \n",
    "          training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.mov')) + list(self.path.glob('*/*.mp4'))\n",
    "        classes = [p.parent.name for p in video_paths] \n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        if self.training:\n",
    "            random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames = frames_from_video_file(path, self.n_frames) \n",
    "            label = self.class_ids_for_name[name] # Encode labels\n",
    "            yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c87af13-9c89-4b1f-9a1b-35879ee1a8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found MOV files: 16\n",
      "Found MP4 files: 23\n",
      "Found CSV files: 40\n",
      "Found 0 video-CSV pairs\n",
      "No matching video-CSV pairs found. Please check the file names and paths.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil  # To move or copy files\n",
    "\n",
    "# Path to your datasets (videos and CSV files)\n",
    "vid_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\Dataset - MP - VID')\n",
    "original_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\Dataset - Original')\n",
    "csv_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\Dataset - MP - CSV')\n",
    "\n",
    "# Find all video files (MOV and MP4) and CSV files\n",
    "mov_files = list(vid_dir.rglob(\"*.mov\"))\n",
    "mp4_files = list(original_dir.rglob(\"*.mp4\"))\n",
    "csv_files = list(csv_dir.rglob(\"*.csv\"))\n",
    "\n",
    "# Print the found video and CSV files for debugging\n",
    "print(f\"Found MOV files: {len(mov_files)}\")\n",
    "print(f\"Found MP4 files: {len(mp4_files)}\")\n",
    "print(f\"Found CSV files: {len(csv_files)}\")\n",
    "\n",
    "# Combine all video files\n",
    "all_video_files = mov_files + mp4_files\n",
    "\n",
    "# Ensure that there is a one-to-one correspondence between video files and CSV files\n",
    "video_to_csv = {}\n",
    "for video_file in all_video_files:\n",
    "    # Check if a CSV file with the same name exists\n",
    "    csv_file = csv_dir / (video_file.stem + '.csv')\n",
    "    if csv_file.exists():\n",
    "        video_to_csv[video_file] = csv_file\n",
    "\n",
    "# Check how many video-to-CSV matches we have\n",
    "print(f\"Found {len(video_to_csv)} video-CSV pairs\")\n",
    "\n",
    "# Now, split data into training, validation, and testing sets if we have valid pairs\n",
    "if len(video_to_csv) > 0:\n",
    "    train_video_files, temp_video_files = train_test_split(list(video_to_csv.keys()), test_size=0.4, random_state=42)\n",
    "    val_video_files, test_video_files = train_test_split(temp_video_files, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Get the corresponding CSV files for the splits\n",
    "    train_csv_files = [video_to_csv[video_file] for video_file in train_video_files]\n",
    "    val_csv_files = [video_to_csv[video_file] for video_file in val_video_files]\n",
    "    test_csv_files = [video_to_csv[video_file] for video_file in test_video_files]\n",
    "\n",
    "    # Print out the number of files in each split to verify\n",
    "    print(\"Training video samples:\", len(train_video_files))\n",
    "    print(\"Validation video samples:\", len(val_video_files))\n",
    "    print(\"Test video samples:\", len(test_video_files))\n",
    "\n",
    "    print(\"Training CSV samples:\", len(train_csv_files))\n",
    "    print(\"Validation CSV samples:\", len(val_csv_files))\n",
    "    print(\"Test CSV samples:\", len(test_csv_files))\n",
    "\n",
    "    # Define directories for the splits\n",
    "    train_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\train')\n",
    "    val_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\val')\n",
    "    test_dir = pathlib.Path(r'C:\\campus\\3rd\\2 sem\\caspone\\sign bridge\\Dataset\\test')\n",
    "\n",
    "    # Ensure that the directories exist, or create them\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    val_dir.mkdir(parents=True, exist_ok=True)\n",
    "    test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Function to move or copy files to the target directory\n",
    "    def move_files(file_list, target_dir):\n",
    "        for file in file_list:\n",
    "            shutil.copy(file, target_dir)  # Or use shutil.move to move the files instead of copying\n",
    "\n",
    "    # Move video files and CSV files to corresponding directories\n",
    "    move_files(train_video_files, train_dir)\n",
    "    move_files(val_video_files, val_dir)\n",
    "    move_files(test_video_files, test_dir)\n",
    "\n",
    "    move_files(train_csv_files, train_dir)\n",
    "    move_files(val_csv_files, val_dir)\n",
    "    move_files(test_csv_files, test_dir)\n",
    "\n",
    "    print(\"Files have been successfully moved to their respective directories.\")\n",
    "else:\n",
    "    print(\"No matching video-CSV pairs found. Please check the file names and paths.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7018dba4-7c57-4fb6-b266-75676a6f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained I3D model (or download one)\n",
    "def build_i3d_model(num_classes):\n",
    "    base_model = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "        pooling='avg'  # Global Average Pooling for reducing the dimensions\n",
    "    )\n",
    "    \n",
    "    # Adding temporal dimension using TimeDistributed layers\n",
    "    inputs = tf.keras.layers.Input(shape=(N_FRAMES, IMAGE_SIZE[0], IMAGE_SIZE[1], 1))\n",
    "\n",
    "    # Convert grayscale frames to 3 channels\n",
    "    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(3, (3, 3), padding=\"same\"))(inputs)\n",
    "\n",
    "    # Feed into I3D model (Inflating ConvNet layers)\n",
    "    x = tf.keras.layers.TimeDistributed(base_model)(x)\n",
    "\n",
    "    # Temporal pooling to aggregate frame-level features\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Classification layer\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.85)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae6c5253-94bf-483c-8a49-b16c9269d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(train_path, val_path, batch_size, n_frames):\n",
    "    # Initialize frame generators for training and validation datasets\n",
    "    train_gen = FrameGenerator(train_path, n_frames, training=True)\n",
    "    val_gen = FrameGenerator(val_path, n_frames, training=False)\n",
    "\n",
    "    # Create TensorFlow Dataset from generators\n",
    "    train_dataset = tf.data.Dataset.from_generator(train_gen,\n",
    "                                                   output_signature=(\n",
    "                                                       tf.TensorSpec(shape=(n_frames, 160, 160, 1), dtype=tf.float32),\n",
    "                                                       tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "    val_dataset = tf.data.Dataset.from_generator(val_gen,\n",
    "                                                 output_signature=(\n",
    "                                                     tf.TensorSpec(shape=(n_frames, 160, 160, 1), dtype=tf.float32),\n",
    "                                                     tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "    \n",
    "    # Apply batching and prefetching to optimize performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Define the function to compile the I3D model\n",
    "def compile_i3d_model(model):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "388dfa44-74c9-4c39-9fb9-faa9461efa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping and ModelCheckpoint callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitors validation loss\n",
    "    patience=7,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'fine_tuned_i3d_ucf101_ssl400.weights.h5',  # Change filename extension\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True  # Ensures only weights are saved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f68375b-d416-42f7-aea9-bf90f53d99a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iterdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FRAMES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mprepare_datasets\u001b[1;34m(train_path, val_path, batch_size, n_frames)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m(train_path, val_path, batch_size, n_frames):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Initialize frame generators for training and validation datasets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     train_gen \u001b[38;5;241m=\u001b[39m \u001b[43mFrameGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     val_gen \u001b[38;5;241m=\u001b[39m FrameGenerator(val_path, n_frames, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Create TensorFlow Dataset from generators\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 86\u001b[0m, in \u001b[0;36mFrameGenerator.__init__\u001b[1;34m(self, path, n_frames, training)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_frames \u001b[38;5;241m=\u001b[39m n_frames\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir()))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_ids_for_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((name, idx) \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'iterdir'"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "train_dataset, val_dataset = prepare_datasets(subset_paths['train'], subset_paths['val'], BATCH_SIZE, N_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588044f-f46b-4b29-abc6-cd5dbd319a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_i3d_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8375e-eb4c-4629-8a49-7e1ce6eaab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd802a7e-689a-4335-99c7-f46bc1b14f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with callbacks\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaac33e-0f92-4c71-ba7d-178c255eccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best training and validation accuracies\n",
    "best_train_acc = max(history.history['accuracy'])\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_train_loss = min(history.history['loss'])\n",
    "best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "print(f'Best Training Accuracy: {best_train_acc:.4f}')\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "print(f'Best Training Loss: {best_train_loss:.4f}')\n",
    "print(f'Best Validation Loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421c3f3-22ac-4d2e-9a82-7a4d590160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "    Plotting training and validation learning curves.\n",
    "\n",
    "    Args:\n",
    "      history: model history with all the metric measures\n",
    "  \"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "  fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  \n",
    "  # Determine upper bound of y-axis\n",
    "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
    "\n",
    "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation']) \n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_ylim([0, 1])\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b428cf7-d962-4f4f-8f5e-c9d85aa8c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset): \n",
    "  \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "  \"\"\"\n",
    "  actual = [labels for _, labels in dataset.unbatch()]\n",
    "  predicted = model.predict(dataset)\n",
    "\n",
    "  actual = tf.stack(actual, axis=0)\n",
    "  predicted = tf.concat(predicted, axis=0)\n",
    "  predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "  return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59042df0-0def-4a83-89dc-78aeab31567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(12, 12)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358fb92-6b1e-419e-aec1-642811c76434",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], N_FRAMES, training=True)\n",
    "labels = list(fg.class_ids_for_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f77f4-a8e5-40b7-8838-e5de8ec94695",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = get_actual_predicted_labels(train_dataset)\n",
    "plot_confusion_matrix(actual, predicted, labels, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f19e82-c7f4-417a-96d9-9a3cd2fc7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_gen = FrameGenerator(subset_paths['test'], N_FRAMES, training=False)\n",
    "test_dataset = tf.data.Dataset.from_generator(test_gen,\n",
    "                                              output_signature=(\n",
    "                                                  tf.TensorSpec(shape=(N_FRAMES, 160, 160, 1), dtype=tf.float32),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647e416-2204-45be-860e-640ede5e24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save('fine_tuned_i3d_ucf101_ssl400.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29664b09-4740-4a9f-a9a6-445d5d291f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "# Function to calculate and print precision, recall, F1 score, and accuracy\n",
    "def print_metrics(actual, predicted):\n",
    "    actual_np = actual.numpy()\n",
    "    predicted_np = predicted.numpy()\n",
    "    \n",
    "    precision = precision_score(actual_np, predicted_np, average='weighted')\n",
    "    recall = recall_score(actual_np, predicted_np, average='weighted')\n",
    "    f1 = f1_score(actual_np, predicted_np, average='weighted')\n",
    "    accuracy = accuracy_score(actual_np, predicted_np)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Example usage\n",
    "fg = FrameGenerator(subset_paths['train'], N_FRAMES, training=True)\n",
    "labels = list(fg.class_ids_for_name.keys())\n",
    "\n",
    "# Assuming `train_dataset` is already prepared\n",
    "actual, predicted = get_actual_predicted_labels(train_dataset)\n",
    "print_metrics(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f346e-4109-46b2-82dc-6ea5747b518d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909c64c-4793-4136-8d68-a7db1a932a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cef557-dc08-420c-aa79-fa327bd10b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
